{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram algorithm\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial you will learn about word vectors. Word vectors are numerical representations of words. Word vectors are a crucial transformation to enable machines to understand text. First you will learn why you need word vectors in natural language processing. Your understanding will then be reinforced with a simple yet colorful example explaining how word vectors make machines to learn text better. Then you will be introduced to a particular word vector algorithm known as **Skip-gram**. You will see how you can use free-flowing text in a supervised framework in order to train a neural network. Afterwords we will be scrutinizing the underlying mechanics of skip-gram by actually implementing the algorithm using Keras. Finally we will conclude the lesson.\n",
    "\n",
    "## Why word vectors?\n",
    "Word vectors are numerical vectors, i.e. sequence of numbers. However they are characterized by a special property,\n",
    "\n",
    "> Words used in similar context will have similar word vectors, whereas words unlikely to be used in the same context will have very different word vectors.\n",
    "\n",
    "In other words, word vectors capture meaning. Words used in a similar context usually have similar semantics, e.g. the word `cat` and `dog` are very similar and often used in very similar contexts, whereas the word `window` is used in a very different context. The context of a word means everything to a word, literally!\n",
    "\n",
    "Due to this reason, word vectors are used as a base transformation for words in almost all modern NLP models. Such as,\n",
    "* Machine translation\n",
    "* Image caption generation\n",
    "* Text generation\n",
    "* Question answering\n",
    "* Chatbots, etc.\n",
    "\n",
    "## I am not very clear how word vectors help NLP models\n",
    "\n",
    "Understandable! We still have not discussed much yet. But by the end of this lesson, you will understand the power of word vectors well. But let us try to grok how word vectors help NLP models with a high-level example. Assume a machine translation model, which looks like below. \n",
    "\n",
    "![Without word vectors](../images/wor2vec_lstm_anim_1.gif)\n",
    "\n",
    "Now let us visualize the same system with word vectors\n",
    "\n",
    "![With word vectors](../images/word2vec_lstm_anim_2.gif)\n",
    "\n",
    "Having word vectors instead of a naive representation like (one-hot encoding) enables the MT model to behave consistently with the language.\n",
    "\n",
    "\n",
    "\n",
    "## What are word vectors?\n",
    "\n",
    "The meaning of a word relies on the context that word is being used. And this is the single most important concept in Word2vec algorithms. This was initially expressed by J.R. Firth; a British linguist.\n",
    "\n",
    "> You shall know a word by the company it keeps - J.R. Firth\n",
    "\n",
    "An illustration will do more justice here.\n",
    "\n",
    "![Word vector sentence](../images/word2vec_sent.png)\n",
    "\n",
    "Here in this *context*, the words `cat` and `dog` are more likely to be used than `window`. Therefore, you can conclude that `cat` and `dog` are more similar than `cat` and `window`, which makes sense. It is also sensible to assign vectors to `cat`, `dog`, and `window` as follows:\n",
    "\n",
    "* `cat` $\\leftarrow (0.41, 0.69, 0.10)$\n",
    "* `dog` $\\leftarrow (0.44, 0.60, 0.15)$\n",
    "* `window` $\\leftarrow (0.01, -0.51, 0.99)$\n",
    "\n",
    "If I plot the above values in a graph, they would look like below.\n",
    "\n",
    "![Word vectors on a graph](../images/word2vec_graph.png)\n",
    "\n",
    "## Overview of Word2vec algorithms\n",
    "\n",
    "Word2vec algorithms; a family of word vector algorithms we will be discussing in this tutorial will have the following structure:\n",
    "* Getting a large corpus of text\n",
    "* Using free-flowing text a supervised framework, i.e., define inputs and outputs from text\n",
    "* Define an embedding layer that will contain word vectors of all the words in the vocabulary\n",
    "* Define a loss function and a neural network to optimize the embedding layer\n",
    "\n",
    "There are several popular word vector algorithms out there. You will be learning Skip-gram algorithm, which is one of the widely used ones.\n",
    "\n",
    "## Skip-gram algorithm\n",
    "\n",
    "You will now learn the Skip-gram algorithm. Skip-gram algorithm which is a Word2vec algorithm follows the above steps in order to learn the word embeddings. And we will see how each step is perfomed through an interactive exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "import requests\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Embedding, Dense, Input\n",
    "from tensorflow.python.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.python.keras.preprocessing.sequence import make_sampling_table\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk # standard preprocessing\n",
    "import operator # sorting items in dictionary by value\n",
    "#from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "#nltk.download('stopwords') #tokenizers/punkt/PY3/english.pickle\n",
    "from math import ceil\n",
    "import csv\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data (if needed)\n",
      "Checking if the file wiki\\MovieSummaries.tar.gz exists\n",
      "\n",
      "\tDone\n",
      "Reading data\n",
      "\n",
      "Data has 10000 documents\n",
      "The first 100 words\n",
      "[\"Shlykov , a hard-working taxi driver and Lyosha , a saxophonist , develop a bizarre love-hate relationship , and despite their prejudices , realize they are n't so different after all .\", \"The nation of Panem consists of a wealthy Capitol and twelve poorer districts . As punishment for a past rebellion , each district must provide a boy and girl between the ages of 12 and 18 selected by lottery for the annual Hunger Games . The tributes must fight to the death in an arena ; the sole survivor is rewarded with fame and wealth . In her first Reaping , 12-year-old Primrose Everdeen is chosen from District 12 . Her older sister Katniss volunteers to take her place . Peeta Mellark , a baker 's son who once gave Katniss bread when she was starving , is the other District 12 tribute . Katniss and Peeta are taken to the Capitol , accompanied by their frequently drunk mentor , past victor Haymitch Abernathy . He warns them about the `` Career '' tributes who train intensively at special academies and almost always win . During a TV interview with Caesar Flickerman , Peeta unexpectedly reveals his love for Katniss . She is outraged , believing it to be a ploy to gain audience support , as `` sponsors '' may provide in-Games gifts of food , medicine , and tools . However , she discovers Peeta meant what he said . The televised Games begin with half of the tributes killed in the first few minutes ; Katniss barely survives ignoring Haymitch 's advice to run away from the melee over the tempting supplies and weapons strewn in front of a structure called the Cornucopia . Peeta forms an uneasy alliance with the four Careers . They later find Katniss and corner her up a tree . Rue , hiding in a nearby tree , draws her attention to a poisonous tracker jacker nest hanging from a branch . Katniss drops it on her sleeping besiegers . They all scatter , except for Glimmer , who is killed by the insects . Hallucinating due to tracker jacker venom , Katniss is warned to run away by Peeta . Rue cares for Katniss for a couple of days until she recovers . Meanwhile , the alliance has gathered all the supplies into a pile . Katniss has Rue draw them off , then destroys the stockpile by setting off the mines planted around it . Furious , Cato kills the boy assigned to guard it . As Katniss runs from the scene , she hears Rue calling her name . She finds Rue trapped and releases her . Marvel , a tribute from District 1 , throws a spear at Katniss , but she dodges the spear , causing it to stab Rue in the stomach instead . Katniss shoots him dead with an arrow . She then comforts the dying Rue with a song . Afterward , she gathers and arranges flowers around Rue 's body . When this is televised , it sparks a riot in Rue 's District 11 . President Snow summons Seneca Crane , the Gamemaker , to express his displeasure at the way the Games are turning out . Since Katniss and Peeta have been presented to the public as `` star-crossed lovers '' , Haymitch is able to convince Crane to make a rule change to avoid inciting further riots . It is announced that tributes from the same district can win as a pair . Upon hearing this , Katniss searches for Peeta and finds him with an infected sword wound in the leg . She portrays herself as deeply in love with him and gains a sponsor 's gift of soup . An announcer proclaims a feast , where the thing each survivor needs most will be provided . Peeta begs her not to risk getting him medicine . Katniss promises not to go , but after he falls asleep , she heads to the feast . Clove ambushes her and pins her down . As Clove gloats , Thresh , the other District 11 tribute , kills Clove after overhearing her tormenting Katniss about killing Rue . He spares Katniss `` just this time ... for Rue '' . The medicine works , keeping Peeta mobile . Foxface , the girl from District 5 , dies from eating nightlock berries she stole from Peeta ; neither knew they are highly poisonous . Crane changes the time of day in the arena to late at night and unleashes a pack of hound-like creatures to speed things up . They kill Thresh and force Katniss and Peeta to flee to the roof of the Cornucopia , where they encounter Cato . After a battle , Katniss wounds Cato with an arrow and Peeta hurls him to the creatures below . Katniss shoots Cato to spare him a prolonged death . With Peeta and Katniss apparently victorious , the rule change allowing two winners is suddenly revoked . Peeta tells Katniss to shoot him . Instead , she gives him half of the nightlock . However , before they can commit suicide , they are hastily proclaimed the victors of the 74th Hunger Games . Haymitch warns Katniss that she has made powerful enemies after her display of defiance . She and Peeta return to District 12 , while Crane is locked in a room with a bowl of nightlock berries , and President Snow considers the situation .\", \"Poovalli Induchoodan is sentenced for six years prison life for murdering his classmate . Induchoodan , the only son of Justice Maranchery Karunakara Menon was framed in the case by Manapally Madhavan Nambiar and his crony DYSP Sankaranarayanan to take revenge on idealist judge Menon who had earlier given jail sentence to Manapally in a corruption case . Induchoodan , who had achieved top rank in Indian Civil Service loses the post and Manapally Sudheeran ( [ [ Saikumar enters the list of civil service trainees . We learn in flashback that it was Ramakrishnan the son of Moopil Nair , who had actually killed his classmate . Six years passes by and Manapally Madhavan Nambiar , now a former state minister , is dead and Induchoodan , who is all rage at the gross injustice meted out to him - thus destroying his promising life , is released from prison . Induchoodan thwarts Manapally Pavithran from performing the funeral rituals of Nambiar at Bharathapuzha . Many confrontations between Induchoodan and Manapally 's henchmen follow . Induchoodan also falls in love with Anuradha ( [ [ Aishwarya , the strong-willed and independent-minded daughter of Mooppil Nair . Justice Menon and his wife returns back to Kerala to stay with Induchoodan . There is an appearance of a girl named Indulekha ( [ [ Kanaka , who claims to be the daughter of Justice Menon . Menon flatly refuses the claim and banishes her . Forced by circumstances and at the instigation and help of Manapally Pavithran , she reluctantly come out open with the claim . Induchoodan at first thrashes the protesters . But upon knowing the truth from Chandrabhanu his uncle , he accepts the task of her protection in the capacity as elder brother . Induchoodan decides to marry off Indulekha to his good friend Jayakrishnan . Induchoodan has a confrontation with his father and prods him to accept mistake and acknowledge the parentage of Indulekha . Menon ultimately regrets and goes on to confess to his daughter . The very next day , when Induchoodan returns to Poovally , Indulekha is found dead and Menon is accused of murdering her . The whole act was planned by Pavithran , who after killing Indulekha , forces Raman Nair to testify against Menon in court . In court , Nandagopal Maarar , a close friend of Induchoodan and a famous supreme court lawyer , appears for Menon and manages to lay bare the murder plot and hidden intentions of other party . Menon is judged innocent of the crime by court . After confronting Pavithran and promising just retribution to the crime of killing Indulekha , Induchoodan returns to his father , who now shows remorse for all his actions including not believing in the innocence of his son . But while speaking to Induchoodan , Menon suffers a heart stroke and passes away . At Menon 's funeral , Manapally Pavithran arrives to poke fun at Induchoodan and he also tries to carry out the postponed last rituals of his own father . Induchoodan interrupts the ritual and avenges for the death of his sister and father by severely injuring Pavithran . On his way back to peaceful life , Induchoodan accepts Anuradha as his life partner .\", \"The Lemon Drop Kid , a New York City swindler , is illegally touting horses at a Florida racetrack . After several successful hustles , the Kid comes across a beautiful , but gullible , woman intending to bet a lot of money . The Kid convinces her to switch her bet , employing a prefabricated con . Unfortunately for the Kid , the woman `` belongs '' to notorious gangster Moose Moran , as does the money . The Kid 's choice finishes dead last and a furious Moran demands the Kid provide him with $ 10,000 by Christmas Eve , or the Kid `` wo n't make it to New Year 's . '' The Kid decides to return to New York to try to come up with the money . He first tries his on-again , off-again girlfriend Brainy Baxter . However , when talk of long-term commitment arises , the Kid quickly makes an escape . He next visits local crime boss `` Oxford '' Charley , with whom he has had past dealings . This falls through as Charley is in serious tax trouble and does not particularly care for the Kid anyway . As he leaves Charley 's establishment and is about to give up hope , the Kid notices a cornerside Santa Claus and his kettle . Thinking quickly , the Kid fashions himself a Santa suit and begins collecting donations . This fails as he is recognized by a passing policeman , who remembers his previous underhanded activity well . The Kid lands in court , where he is convicted of collecting for a charity without a license and sentenced to ten days in jail . However , while in court , the Kid learns where his scheme went wrong . After a short stay , Brainy arrives to bail him out . He then sets about restarting his Santa operation , this time with legitimate backing . To this end , he needs a charity to represent and a city license . The kid receives key inspiration when he remembers that Nellie Thursday , a kindly neighborhood resident , has been denied entry to a retirement home because of her jailed husband 's criminal past as a safecracker . Organizing other small-time New York swindlers and Brainy , who is both surprised and charmed at the Kid 's apparent goodwill , the Kid converts an abandoned casino into the `` Nellie Thursday Home For Old Dolls '' . A small group of elderly women and makeshift amenities complete the project . The Kid is able to receive the all-important city license . Now free to collect , the Kid and his compatriots dress as Santa Claus and position themselves throughout Manhattan . The others are unaware that the Kid plans to keep the money for himself to pay off Moran . The scheme is a huge success , netting $ 2,000 in only a few days . An overjoyed Brainy decides to leave her job as a dancer and look after the `` home '' full-time until after Christmas . Coincidentally , her employer is none other than `` Oxford '' Charley , whom Brainy cheerfully informs of the effort . Seeing a potential gold mine , Charley decides to muscle in on the operation . Reasoning that the Nellie Thursday home is `` wherever Nellie Thursday is '' , Charley and his crew kidnap the home 's inhabitants and move them to Charley 's mansion in Nyack . The Kid learns of this when he returns to the home after a late night to find the home deserted and money gone . Clued in by oversized Oxford footprints in the snow , the Kid and his friends pay Charley a visit . Here , Charley reveals the true nature of the Kid 's scheme through a phone conversation with Moose Moran . The Kid 's accomplices are angry and move to confront him , but the Kid manages to slip away . However , Brainy tracks him down outside and voices her disgust at his actions . After a few days of stewing in self-pity , the Kid is surprised to meet Nellie , who has escaped Charley 's compound . He decides to recover the money , sneaking into Charley 's home in the guise of an elderly woman . He finds that Charley and his crew are again moving the women , this time to a more secure location . Using the heightened activity to his advantage , the Kid enters Charley 's office and confronts him . After a brief struggle , the Kid overpowers Charley and makes off with the money , narrowly avoiding the thugs Charley has sent after him . The ensuing chaos allows Brainy and the others to escape . Later that night , the Kid returns to the original Nellie Thursday home to meet with Moose Moran . The deal appears to be in jeopardy as Moran arrives with Charley . Charley demands that the Kid reimburse him , which would leave too little for Moran . However , the Kid turns the tables by hitting a switch , revealing hidden casino tables . All are occupied , mainly by the escaped old dolls . The Kid and his still-loyal friends hold off the gangsters as the police initiate a raid . Moran and Charley are arrested while the judge who sentenced the Kid earlier warns that he will be `` keeping an eye on him '' . The Kid assures him that will not be necessary and his attention will lie on the home , which is going to become a reality . The night 's main event begins as Nellie 's husband Henry , free on parole , joyously reunites with his wife .\", \"Seventh-day Adventist Church pastor Michael Chamberlain , his wife Lindy , their two sons , and their nine-week-old daughter Azaria are on a camping holiday in the Outback . With the baby sleeping in their tent , the family is enjoying a barbecue with their fellow campers when a cry is heard . Lindy returns to the tent to check on Azaria and is certain she sees a dingo with something in its mouth running off as she approaches . When she discovers the infant is missing , everyone joins forces to search for her , without success . It is assumed what Lindy saw was the animal carrying off the child , and a subsequent inquest rules her account of events is true . The tide of public opinion soon turns against the Chamberlains . For many , Lindy seems too stoic , too cold-hearted , and too accepting of the disaster that has befallen her . Gossip about her begins to swell and soon is accepted as statements of fact . The couple 's beliefs are not widely practised in the country , and when the media report a rumour that the name Azaria means `` sacrifice in the wilderness '' , the public is quick to believe they decapitated their baby with a pair of scissors as part of a bizarre religious rite . Law-enforcement officials find new witnesses , forensics experts , and a lot of circumstantial evidence—including a small wooden coffin Michael uses as a receptacle for his parishioners ' packs of un-smoked cigarettes—and reopen the investigation , and eventually Lindy is charged with murder . Seven months pregnant , she ignores her attorneys ' advice to play on the jury 's sympathy and appears emotionless on the stand , convincing onlookers she is guilty of the crime of which she is accused . As the trial progresses , Michael 's faith in his religion and his belief in his wife disintegrate , and he stumbles through his testimony , suggesting he is concealing the truth . In October 1982 , Lindy is found guilty and sentenced to life imprisonment with hard labour , while Michael is found guilty as an accessory and given an 18-month suspended sentence . More than three years later , while searching for the body of an English tourist who fell from Uluru , police discover a small item of clothing that is identified as the jacket Lindy had insisted Azaria was wearing over her jumpsuit , which had been recovered early in the investigation . She is immediately released from prison , the case reopened and all convictions against the Chamberlains overturned .\"]\n",
      "\n",
      "The last 100 words\n",
      "[\"Yoon-woon works in his mother 's restaurant , and is more interested in having a good time with his friends than settling down with his fiancée . He allows himself to be seduced by bargirl Yeon-ah , and the two embark on a tumultuous love-hate relationship . But when Young-woon 's mother finds out about the affair and pushes him into marrying his fiancée , he is forced into choosing between the two women .\", \"In a working-class district of Paris , Albert , an impecunious street singer , lives in an attic room . He meets a beautiful Romanian girl , Pola , and falls in love with her ; but he is not the only one , since his best friend Louis and the gangster Fred are also under her spell . One evening Pola dares not return home because Fred has stolen her key and she does not feel safe . She spends the night with Albert who , reluctantly remaining the gentleman , sleeps on the floor and leaves his bed to Pola . They soon decide to get married , but fate prevents them when Émile , a thief , deposits with Albert a bag full of stolen goods . It is discovered by the police , and Albert is sent to prison . Pola finds consolation with Louis . Later Émile is caught in his turn and admits that Albert was not his accomplice , which earns Albert his freedom . Fred has just got back together with Pola who has fallen out with Louis , and in a jealous fury at Albert 's return Fred decides to provoke a knife fight with him . Louis rushes to Albert 's rescue and the two comrades are re-united , but their friendship is clouded by the realisation that each of them is in love with Pola . Finally Albert decides to give up Pola to Louis .\", 'A second generation British Asian accountant wants to be a film producer . He has an idea for a film , called Ealing Comedy about an accountant turned film producer called Alfie Singh . Alfie will play himself and his real son , Paul , will play his son in the film . Unable to raise finance he decides to make the film himself . The film chronicles his life with his Irish wife and teenage son and his struggles to finance and make the film while keeping his family together .', 'When Arlena Twigg becomes ill , a blood test reveals that she is not the biological daughter of Regina and Ernest Twigg . When Arlena dies at the age of nine , her parents search for their biological daughter who is being raised as Kimberly Mays .', 'In 1830 , at a finishing school in Styria , Mircalla arrives as a new student . A visiting author , Richard Lestrange , instantly falls in love with her ; but Mircalla is a vampire & mdash ; Carmilla Karnstein & mdash ; who has been resurrected by her vampiric family . As students in the school , inhabitants of the nearby village and those who suspect Mircalla is responsible start to die , suspicion turns toward the Karnsteins and their ominous castle .']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\" Reading the zip file to extract text \"\"\"\n",
    "    docs = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for row in f:\n",
    "            file_string = nltk.word_tokenize(row)\n",
    "            # First token is the movie ID\n",
    "            docs.append(' '.join(file_string[1:]))\n",
    "    return docs\n",
    "    \n",
    "    \n",
    "def download_wiki(url):\n",
    "    \"\"\" Download the data if they are not found \"\"\"\n",
    "    f = os.path.join('wiki', \"MovieSummaries.tar.gz\")\n",
    "    try:\n",
    "        print('Checking if the file {} exists'.format(f))\n",
    "        \n",
    "        assert_msg = 'Some of the files were missing'\n",
    "        assert os.path.exists(f), assert_msg\n",
    "            \n",
    "    except:\n",
    "        print(\"Unable to find the file {}\".format(f))\n",
    "        print('Downloading Wiki data from {}'.format(url))\n",
    "        res = requests.get(url, stream=True)\n",
    "\n",
    "        total_length = int(res.headers.get('content-length')) \\\n",
    "            if res.headers.get('content-length') is not None else None\n",
    "        \n",
    "        print('Detected data size: {}KB'.format(total_length))\n",
    "        print('Making a directory cifar-10 to store data')\n",
    "        if not os.path.exists('wiki'):\n",
    "            os.mkdir('wiki')\n",
    "        with open(f, 'wb') as f:\n",
    "            print('Downloading data')\n",
    "            for data in res.iter_content(chunk_size=1024*1024):\n",
    "                print('.',end='')\n",
    "                f.write(data)\n",
    "        \n",
    "\n",
    "        \n",
    "    print('\\n\\tDone')\n",
    "\n",
    "print('Downloading data (if needed)')\n",
    "download_wiki('http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz')\n",
    "print('Reading data')\n",
    "docs = read_data(os.path.join('wiki','plot_summaries.txt',))\n",
    "\n",
    "print('\\nData has {} documents'.format(len(docs)))\n",
    "print('The first 100 words')\n",
    "print(docs[:5])\n",
    "print('\\nThe last 100 words')\n",
    "print(docs[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the tokenizer\n",
      "shlykov word becomes [1] when tokenized\n",
      "Maximum token: 69689\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 25000\n",
    "def get_tokenizer(docs, vocab_size):\n",
    "    \"\"\" Getting the Keras tokenizer trained with data \"\"\"\n",
    "    \n",
    "    # TODO: Define a tokenizer which has vocab_size number of words and out of vocabulary token \"UNK\"\n",
    "    # assign this tokenizer to the variable toc\n",
    "    tok = Tokenizer(num_words=vocab_size, oov_token='UNK')\n",
    "    # TODO: fit the tokenizer on docs\n",
    "    tok.fit_on_texts(docs)\n",
    "    return tok\n",
    "\n",
    "print('Creating the tokenizer')\n",
    "tok = get_tokenizer(docs, vocab_size)\n",
    "index2word = dict(zip(tok.word_index.values(), tok.word_index.keys()))\n",
    "print('{} word becomes {} when tokenized'.format('shlykov',tok.texts_to_sequences(['shlykov'])[0]))\n",
    "print('Maximum token: {}'.format(max(list(tok.word_index.values()))))\n",
    "print('\\tDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words: \n",
      "\t['the', 'to', 'and', 'a', 'of', 'is', 'in', 'his', 'he', 'her']\n",
      "\n",
      "Least frequent words: \n",
      "\t['1526', 'landsknechts', 'frundsberg', 'falconet', 'unassumingly', 'straightforwardly', 'bargirl', 'styria', 'lestrange', 'karnsteins']\n"
     ]
    }
   ],
   "source": [
    "sorted_words = sorted(list(tok.word_counts.items()), key=lambda x: x[1], reverse=True)\n",
    "print('Most frequent words: \\n\\t{}'.format([w for w, _ in sorted_words[:10]]))\n",
    "print('\\nLeast frequent words: \\n\\t{}'.format([w for w, _ in sorted_words[-10:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How `skipgrams` work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating inputs outputs using the sentence \n",
      "\tShlykov , a hard-working taxi driver and Lyosha , a saxophonist , develop a bizarre love-hate relationship , and despite their prejudices , realize they are n't so different after all .\n",
      "\n",
      "Text after fitting through the tokenizer \n",
      "\tUNK a hard working taxi driver and UNK a saxophonist develop a bizarre love hate relationship and despite their prejudices realize they are n't so different after all\n",
      "\n",
      "\n",
      " Length:  204\n",
      "\t=> realize / prejudices =>\t label(1/0) => 1\n",
      "\t=> working / taxi =>\t label(1/0) => 1\n",
      "\t=> taxi / UNK =>\t label(1/0) => 1\n",
      "\t=> are / prejudices =>\t label(1/0) => 1\n",
      "\t=> and / hard =>\t label(1/0) => 1\n",
      "\t=> develop / saxophonist =>\t label(1/0) => 1\n",
      "\t=> saxophonist / love =>\t label(1/0) => 1\n",
      "\t=> despite / prejudices =>\t label(1/0) => 1\n",
      "\t=> all / so =>\t label(1/0) => 1\n",
      "\t=> bizarre / a =>\t label(1/0) => 1\n",
      "\t=> working / driver =>\t label(1/0) => 1\n",
      "\t=> develop / UNK =>\t label(1/0) => 1\n",
      "\t=> and / love =>\t label(1/0) => 1\n",
      "\t=> hate / a =>\t label(1/0) => 1\n",
      "\t=> n't / realize =>\t label(1/0) => 1\n",
      "\t=> a / taxi =>\t label(1/0) => 1\n",
      "\t=> bizarre / a =>\t label(1/0) => 1\n",
      "\t=> prejudices / they =>\t label(1/0) => 1\n",
      "\t=> a / a =>\t label(1/0) => 1\n",
      "\t=> UNK / working =>\t label(1/0) => 1\n",
      "\t=> UNK / a =>\t label(1/0) => 1\n",
      "\t=> so / after =>\t label(1/0) => 1\n",
      "\t=> a / bizarre =>\t label(1/0) => 1\n",
      "\t=> are / they =>\t label(1/0) => 1\n",
      "\t=> a / driver =>\t label(1/0) => 1\n",
      "\t=> and / despite =>\t label(1/0) => 1\n",
      "\t=> all / different =>\t label(1/0) => 1\n",
      "\t=> and / driver =>\t label(1/0) => 1\n",
      "\t=> n't / are =>\t label(1/0) => 1\n",
      "\t=> love / saxophonist =>\t label(1/0) => 1\n",
      "\t=> different / n't =>\t label(1/0) => 1\n",
      "\t=> they / different =>\t label(1/0) => 1\n",
      "\t=> taxi / hard =>\t label(1/0) => 1\n",
      "\t=> working / UNK =>\t label(1/0) => 1\n",
      "\t=> develop / a =>\t label(1/0) => 1\n",
      "\t=> after / all =>\t label(1/0) => 1\n",
      "\t=> develop / love =>\t label(1/0) => 1\n",
      "\t=> different / after =>\t label(1/0) => 1\n",
      "\t=> driver / saxophonist =>\t label(1/0) => 1\n",
      "\t=> so / they =>\t label(1/0) => 1\n",
      "\t=> despite / their =>\t label(1/0) => 1\n",
      "\t=> they / prejudices =>\t label(1/0) => 1\n",
      "\t=> n't / after =>\t label(1/0) => 1\n",
      "\t=> their / are =>\t label(1/0) => 1\n",
      "\t=> so / all =>\t label(1/0) => 1\n",
      "\t=> different / so =>\t label(1/0) => 1\n",
      "\t=> a / saxophonist =>\t label(1/0) => 1\n",
      "\t=> love / relationship =>\t label(1/0) => 1\n",
      "\t=> so / n't =>\t label(1/0) => 1\n",
      "\t=> despite / realize =>\t label(1/0) => 1\n",
      "\t=> realize / n't =>\t label(1/0) => 1\n",
      "\t=> hate / love =>\t label(1/0) => 1\n",
      "\t=> prejudices / realize =>\t label(1/0) => 1\n",
      "\t=> UNK / and =>\t label(1/0) => 1\n",
      "\t=> a / UNK =>\t label(1/0) => 1\n",
      "\t=> so / realize =>\t label(1/0) => 1\n",
      "\t=> hard / a =>\t label(1/0) => 1\n",
      "\t=> they / so =>\t label(1/0) => 1\n",
      "\t=> different / all =>\t label(1/0) => 1\n",
      "\t=> relationship / hate =>\t label(1/0) => 1\n",
      "\t=> relationship / a =>\t label(1/0) => 1\n",
      "\t=> after / are =>\t label(1/0) => 1\n",
      "\t=> and / a =>\t label(1/0) => 1\n",
      "\t=> their / they =>\t label(1/0) => 1\n",
      "\t=> driver / taxi =>\t label(1/0) => 1\n",
      "\t=> love / and =>\t label(1/0) => 1\n",
      "\t=> saxophonist / and =>\t label(1/0) => 1\n",
      "\t=> UNK / a =>\t label(1/0) => 1\n",
      "\t=> relationship / love =>\t label(1/0) => 1\n",
      "\t=> a / develop =>\t label(1/0) => 1\n",
      "\t=> UNK / hard =>\t label(1/0) => 1\n",
      "\t=> driver / hard =>\t label(1/0) => 1\n",
      "\t=> so / different =>\t label(1/0) => 1\n",
      "\t=> hate / their =>\t label(1/0) => 1\n",
      "\t=> realize / and =>\t label(1/0) => 1\n",
      "\t=> a / driver =>\t label(1/0) => 1\n",
      "\t=> love / despite =>\t label(1/0) => 1\n",
      "\t=> n't / different =>\t label(1/0) => 1\n",
      "\t=> hard / and =>\t label(1/0) => 1\n",
      "\t=> hate / despite =>\t label(1/0) => 1\n",
      "\t=> n't / they =>\t label(1/0) => 1\n",
      "\t=> develop / a =>\t label(1/0) => 1\n",
      "\t=> driver / a =>\t label(1/0) => 1\n",
      "\t=> driver / and =>\t label(1/0) => 1\n",
      "\t=> despite / hate =>\t label(1/0) => 1\n",
      "\t=> realize / despite =>\t label(1/0) => 1\n",
      "\t=> prejudices / n't =>\t label(1/0) => 1\n",
      "\t=> taxi / a =>\t label(1/0) => 1\n",
      "\t=> hate / relationship =>\t label(1/0) => 1\n",
      "\t=> develop / and =>\t label(1/0) => 1\n",
      "\t=> UNK / taxi =>\t label(1/0) => 1\n",
      "\t=> a / saxophonist =>\t label(1/0) => 1\n",
      "\t=> n't / so =>\t label(1/0) => 1\n",
      "\t=> taxi / UNK =>\t label(1/0) => 1\n",
      "\t=> their / despite =>\t label(1/0) => 1\n",
      "\t=> and / saxophonist =>\t label(1/0) => 1\n",
      "\t=> prejudices / relationship =>\t label(1/0) => 1\n",
      "\t=> working / hard =>\t label(1/0) => 1\n",
      "\t=> different / they =>\t label(1/0) => 1\n",
      "\t=> love / develop =>\t label(1/0) => 1\n",
      "\t=> a / UNK =>\t label(1/0) => 1\n",
      "\t=> a / and =>\t label(1/0) => 1\n",
      "\t=> UNK / taxi =>\t label(1/0) => 1\n",
      "\t=> despite / and =>\t label(1/0) => 1\n",
      "\t=> and / develop =>\t label(1/0) => 1\n",
      "\t=> saxophonist / bizarre =>\t label(1/0) => 1\n",
      "\t=> hard / working =>\t label(1/0) => 1\n",
      "\t=> their / relationship =>\t label(1/0) => 1\n",
      "\t=> are / so =>\t label(1/0) => 1\n",
      "\t=> hate / and =>\t label(1/0) => 1\n",
      "\t=> relationship / bizarre =>\t label(1/0) => 1\n",
      "\t=> bizarre / love =>\t label(1/0) => 1\n",
      "\t=> driver / UNK =>\t label(1/0) => 1\n",
      "\t=> prejudices / are =>\t label(1/0) => 1\n",
      "\t=> UNK / a =>\t label(1/0) => 1\n",
      "\t=> realize / are =>\t label(1/0) => 1\n",
      "\t=> different / are =>\t label(1/0) => 1\n",
      "\t=> and / taxi =>\t label(1/0) => 1\n",
      "\t=> a / relationship =>\t label(1/0) => 1\n",
      "\t=> and / their =>\t label(1/0) => 1\n",
      "\t=> a / taxi =>\t label(1/0) => 1\n",
      "\t=> develop / hate =>\t label(1/0) => 1\n",
      "\t=> saxophonist / a =>\t label(1/0) => 1\n",
      "\t=> bizarre / and =>\t label(1/0) => 1\n",
      "\t=> a / working =>\t label(1/0) => 1\n",
      "\t=> a / a =>\t label(1/0) => 1\n",
      "\t=> prejudices / their =>\t label(1/0) => 1\n",
      "\t=> realize / they =>\t label(1/0) => 1\n",
      "\t=> taxi / and =>\t label(1/0) => 1\n",
      "\t=> bizarre / hate =>\t label(1/0) => 1\n",
      "\t=> realize / so =>\t label(1/0) => 1\n",
      "\t=> a / bizarre =>\t label(1/0) => 1\n",
      "\t=> working / a =>\t label(1/0) => 1\n",
      "\t=> after / n't =>\t label(1/0) => 1\n",
      "\t=> saxophonist / develop =>\t label(1/0) => 1\n",
      "\t=> bizarre / relationship =>\t label(1/0) => 1\n",
      "\t=> realize / their =>\t label(1/0) => 1\n",
      "\t=> taxi / working =>\t label(1/0) => 1\n",
      "\t=> they / their =>\t label(1/0) => 1\n",
      "\t=> relationship / prejudices =>\t label(1/0) => 1\n",
      "\t=> bizarre / saxophonist =>\t label(1/0) => 1\n",
      "\t=> hard / taxi =>\t label(1/0) => 1\n",
      "\t=> relationship / and =>\t label(1/0) => 1\n",
      "\t=> saxophonist / a =>\t label(1/0) => 1\n",
      "\t=> despite / they =>\t label(1/0) => 1\n",
      "\t=> their / prejudices =>\t label(1/0) => 1\n",
      "\t=> hate / bizarre =>\t label(1/0) => 1\n",
      "\t=> are / different =>\t label(1/0) => 1\n",
      "\t=> taxi / driver =>\t label(1/0) => 1\n",
      "\t=> n't / all =>\t label(1/0) => 1\n",
      "\t=> UNK / driver =>\t label(1/0) => 1\n",
      "\t=> their / and =>\t label(1/0) => 1\n",
      "\t=> they / are =>\t label(1/0) => 1\n",
      "\t=> bizarre / develop =>\t label(1/0) => 1\n",
      "\t=> so / are =>\t label(1/0) => 1\n",
      "\t=> and / UNK =>\t label(1/0) => 1\n",
      "\t=> develop / bizarre =>\t label(1/0) => 1\n",
      "\t=> UNK / working =>\t label(1/0) => 1\n",
      "\t=> prejudices / despite =>\t label(1/0) => 1\n",
      "\t=> they / realize =>\t label(1/0) => 1\n",
      "\t=> a / develop =>\t label(1/0) => 1\n",
      "\t=> and / hate =>\t label(1/0) => 1\n",
      "\t=> and / realize =>\t label(1/0) => 1\n",
      "\t=> hard / driver =>\t label(1/0) => 1\n",
      "\t=> prejudices / and =>\t label(1/0) => 1\n",
      "\t=> a / hard =>\t label(1/0) => 1\n",
      "\t=> after / different =>\t label(1/0) => 1\n",
      "\t=> after / so =>\t label(1/0) => 1\n",
      "\t=> all / n't =>\t label(1/0) => 1\n",
      "\t=> love / a =>\t label(1/0) => 1\n",
      "\t=> their / hate =>\t label(1/0) => 1\n",
      "\t=> are / their =>\t label(1/0) => 1\n",
      "\t=> a / UNK =>\t label(1/0) => 1\n",
      "\t=> hate / develop =>\t label(1/0) => 1\n",
      "\t=> and / working =>\t label(1/0) => 1\n",
      "\t=> love / bizarre =>\t label(1/0) => 1\n",
      "\t=> a / hate =>\t label(1/0) => 1\n",
      "\t=> despite / love =>\t label(1/0) => 1\n",
      "\t=> hard / UNK =>\t label(1/0) => 1\n",
      "\t=> driver / a =>\t label(1/0) => 1\n",
      "\t=> saxophonist / UNK =>\t label(1/0) => 1\n",
      "\t=> working / UNK =>\t label(1/0) => 1\n",
      "\t=> despite / relationship =>\t label(1/0) => 1\n",
      "\t=> are / realize =>\t label(1/0) => 1\n",
      "\t=> UNK / develop =>\t label(1/0) => 1\n",
      "\t=> saxophonist / driver =>\t label(1/0) => 1\n",
      "\t=> driver / working =>\t label(1/0) => 1\n",
      "\t=> love / hate =>\t label(1/0) => 1\n",
      "\t=> and / prejudices =>\t label(1/0) => 1\n",
      "\t=> n't / prejudices =>\t label(1/0) => 1\n",
      "\t=> UNK / saxophonist =>\t label(1/0) => 1\n",
      "\t=> relationship / despite =>\t label(1/0) => 1\n",
      "\t=> are / after =>\t label(1/0) => 1\n",
      "\t=> a / love =>\t label(1/0) => 1\n",
      "\t=> and / relationship =>\t label(1/0) => 1\n",
      "\t=> are / n't =>\t label(1/0) => 1\n",
      "\t=> and / bizarre =>\t label(1/0) => 1\n",
      "\t=> their / realize =>\t label(1/0) => 1\n",
      "\t=> all / after =>\t label(1/0) => 1\n",
      "\t=> they / n't =>\t label(1/0) => 1\n",
      "\t=> working / and =>\t label(1/0) => 1\n",
      "\t=> taxi / a =>\t label(1/0) => 1\n",
      "\t=> they / despite =>\t label(1/0) => 1\n",
      "\t=> relationship / their =>\t label(1/0) => 1\n"
     ]
    }
   ],
   "source": [
    "print('Creating inputs outputs using the sentence \\n\\t{}'.format(docs[0]))\n",
    "\n",
    "# TODO: Create a sequence from first document \n",
    "# ([docs[0]]) using the tokenizer and assign it to seq\n",
    "seq = tok.texts_to_sequences([docs[0]])[0]\n",
    "print('\\nText after fitting through the tokenizer \\n\\t{}\\n'.format(' '.join([index2word[wid] for wid in seq])))\n",
    "\n",
    "# TODO: use the function skipgrams, with sequence of text, \n",
    "# negative_samples and vocabulary_size \n",
    "wpairs, labels = skipgrams(\n",
    "    seq, vocabulary_size=vocab_size, negative_samples=0.0)\n",
    "print('\\n Length: ', len(wpairs))\n",
    "for wp, lbl in zip(wpairs, labels):\n",
    "    print('\\t=> {} / {} =>\\t label(1/0) => {}'.format(index2word[wp[0]], index2word[wp[1]], lbl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a neural network to learn embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 96)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "emb_size = 96\n",
    "num_sampled = 32\n",
    "batch_size = 64\n",
    "window_size = 4\n",
    "\n",
    "def build_graph(vocab_size, embedding_size, batch_size=None):\n",
    "    \"\"\" \n",
    "    This function builds the computation graph. \n",
    "    1. Define a placeholder to take inputs\n",
    "    2. Define embedding matrix, softmax weights, softmax biases\n",
    "    3. Lookup an embedding matrix for the indices given in inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Define a placeholder to feed in indices of words and call it dataset (type tf.int32)\n",
    "    dataset = tf.placeholder(\n",
    "        name=\"input\", shape=[None], dtype=tf.int32\n",
    "    )\n",
    "    \"\"\" Parameters \"\"\"\n",
    "    \n",
    "    # TODO: Define an embedding variable of size [vocab_size, embedding_size] and assign it to embeddings\n",
    "    embeddings = tf.get_variable(\n",
    "        'embeddings', shape=[vocab_size, emb_size],\n",
    "        initializer=tf.glorot_uniform_initializer\n",
    "    )\n",
    "    # Softmax Weights and Biases\n",
    "    \n",
    "    # TODO: Define a TF variable called softmax_weights, define shape and initialize with tf.glorot_uniform_initializer\n",
    "    softmax_weights = tf.get_variable(\n",
    "        'weights', shape=[vocab_size,emb_size], \n",
    "        initializer=tf.glorot_uniform_initializer\n",
    "    )\n",
    "    # TODO: Define a TF variable called softmax_bias, define shape and initialize with tf.glorot_uniform_initializer\n",
    "    softmax_bias = tf.get_variable(\n",
    "        'bias', shape=[vocab_size],\n",
    "        initializer=tf.glorot_uniform_initializer\n",
    "    )\n",
    "    \"\"\" Embedding lookup \"\"\"\n",
    "    \n",
    "    # TODO: get the embeddings for the word indices fed in the dataset and assign it to embed\n",
    "    embed = tf.nn.embedding_lookup(embeddings, dataset)\n",
    "    print(embed.shape)\n",
    "    return dataset, embed\n",
    "    \n",
    "\n",
    "def get_cosine_sim(tf_in, embeddings, top_k):\n",
    "    \"\"\"Compute the similarity between minibatch examples and all embeddings.\n",
    "    We use the cosine distance.\"\"\"\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    in_embeds = tf.nn.embedding_lookup(normalized_embeddings, tf_in)\n",
    "    similarity = tf.matmul(in_embeds, tf.transpose(normalized_embeddings))\n",
    "    val, ind = tf.math.top_k(similarity, k=top_k, sorted=True)\n",
    "\n",
    "    return ind,val\n",
    "\n",
    "\n",
    "tf_in, tf_embeds = build_graph(vocab_size, emb_size)\n",
    "\n",
    "\"\"\" Defining labels\"\"\"\n",
    "tf_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "\"\"\" Defining loss \"\"\"\n",
    "with tf.variable_scope('', reuse=True):\n",
    "    \n",
    "    # Use tf.nn.sampled_softmax_loss to compute the loss and assign it to tf_loss\n",
    "    tf_loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights=tf.get_variable('weights'),\n",
    "        biases=tf.get_variable('bias'),\n",
    "        num_sampled=num_sampled,\n",
    "        num_classes=vocab_size,\n",
    "        inputs=tf_embeds,\n",
    "        labels=tf_labels\n",
    "    ))\n",
    "    \"\"\" Defining similar word computation \"\"\"\n",
    "    tf_best_inds, tf_best_vals = get_cosine_sim(tf_in, tf.get_variable('embeddings'),9)\n",
    "\n",
    "\"\"\" Defining optimization \"\"\"\n",
    "\n",
    "# Use tf.train.AdaGradOptimizer with learning_rate 1.0 \n",
    "# and assign it to tf_opt\n",
    "tf_opt = tf.train.AdagradOptimizer(\n",
    "    learning_rate=1.0).minimize(tf_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and evaluating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 3.7254695892333984\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: car, woods, new, being, hindu, also, reena, now\n",
      "apartment: martyr, valuable, owner, murdered, front, chase, anything, arranged\n",
      "in: soldiers, as, between, film, young, girl, state, group\n",
      "was: car, where, bear, megan, now, trap, been, after\n",
      "UNK: chandru, piece, jaan, paints, named, all, former, a\n",
      "panics: maneuvers, hash, waller, misleads, wreaks, fievel, resemblance, immerses\n",
      "lead: saket, reena, also, ryan, do, frank, india, meets\n",
      "former: named, street, search, 's, faiz, all, which, who\n",
      "to: face, death, never, boyfriend, off, fever, how, friends\n",
      "and: with, service, is, daughter, young, by, job, sons\n",
      "two: while, opens, one, british, into, town, story, police\n",
      "way: city, lives, police, place, original, car, ends, town\n",
      "UNK: chandru, piece, jaan, paints, named, all, former, a\n",
      "manages: malu, keep, friends, family, chandran, rescue, back, decide\n",
      "from: india, new, car, up, first, reena, megan, couple\n",
      "have: car, woods, new, being, hindu, also, reena, now\n",
      "eva: vel, rrts, vesper, pilate, anthea, stain, interrogations, inclined\n",
      "an: all, a, 's, franco, '', officer, faiz, group\n",
      "former: named, street, search, 's, faiz, all, which, who\n",
      "to: face, death, never, boyfriend, off, fever, how, friends\n",
      "============================== \n",
      "\n",
      "Loss in epoch 1: 3.3757548332214355\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: together, watch, get, see, meet, shut, start, present\n",
      "apartment: martyr, follows, valuable, north, first, serious, anything, years\n",
      "in: with, on, from, of, requires, after, at, various\n",
      "was: graham, saying, captured, himself, wind, humans, inherit, girl\n",
      "UNK: the, as, from, popular, requires, enter, of, in\n",
      "panics: maneuvers, hash, waller, misleads, wreaks, fievel, resemblance, immerses\n",
      "lead: saket, reena, ryan, months, frank, dies, gang, lady\n",
      "former: named, mistaken, street, search, political, run, worker, living\n",
      "to: on, by, for, heart, transplant, jack, paraphernalia, as\n",
      "and: transplant, nero, for, 's, in, a, the, after\n",
      "two: men, final, around, illegal, during, under, takes, place\n",
      "way: next, sea, turns, last, end, minion, before, spare\n",
      "UNK: the, as, from, popular, requires, enter, of, in\n",
      "manages: prevent, manage, order, try, window, goes, down, save\n",
      "from: as, at, requires, courage, —, paraphernalia, scheme, exposing\n",
      "have: together, watch, get, see, meet, shut, start, present\n",
      "eva: vel, rrts, vesper, pilate, anthea, stain, interrogations, inclined\n",
      "an: security, rome, american, insistence, appears, occupied, playing, '\n",
      "former: named, mistaken, street, search, political, run, worker, living\n",
      "to: on, by, for, heart, transplant, jack, paraphernalia, as\n",
      "============================== \n",
      "\n",
      "Loss in epoch 2: 3.2457354068756104\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: n't, declines, will, seeing, refuses, ca, accompany, accompanied\n",
      "apartment: medical, roadside, times, government, uncertain, laboratories, farm, encounters\n",
      "in: at, speed, benjie, which, misses, for, undergoing, john\n",
      "was: is, had, day, fake, disease, age, physician, proved\n",
      "UNK: for, annoyed, with, ryu, gets, from, but, sebastian\n",
      "panics: maneuvers, hash, waller, misleads, wreaks, fievel, immerses, flagged\n",
      "lead: saket, frank, ryan, ayan, radha, caravaggio, dies, reena\n",
      "former: young, named, 16, gay, thousand, mistaken, professional, woman\n",
      "to: on, out, down, initially, declines, ken, upcoming, into\n",
      "and: who, by, UNK, which, for, the, 's, of\n",
      "two: days, outside, men, shows, around, follows, during, factory\n",
      "way: empire, next, crowd, boys, beach, members, secrets, british\n",
      "UNK: for, annoyed, with, ryu, gets, from, but, sebastian\n",
      "manages: try, stay, attempts, save, help, bring, use, bodies\n",
      "from: into, letter, by, for, test, john, arnold, shaft\n",
      "have: n't, declines, will, seeing, refuses, ca, accompany, accompanied\n",
      "eva: vel, rrts, vesper, pilate, anthea, stain, interrogations, inclined\n",
      "an: huge, sakura, a, willoughby, transferred, bachchan, distracted, agents\n",
      "former: young, named, 16, gay, thousand, mistaken, professional, woman\n",
      "to: on, out, down, initially, declines, ken, upcoming, into\n",
      "============================== \n",
      "\n",
      "Loss in epoch 3: 3.2687416076660156\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: could, hire, become, loved, about, interned, n't, suggesting\n",
      "apartment: uses, discover, lunch, childhood, teaching, grocery, hears, unknown\n",
      "in: UNK, argentine, the, eleventh, centres, tradition, war, climax\n",
      "was: user, had, day, also, reveals, discovers, lured, night\n",
      "UNK: in, the, aristocratic, aim, mules, on, centres, biker\n",
      "panics: maneuvers, hash, misleads, wreaks, fievel, immerses, flagged, rental\n",
      "lead: events, care, fatal, opposition, ones, k, lovers, large\n",
      "former: young, between, businessman, singer, fairy, half, stalked, heart\n",
      "to: ecstasy, and, hammers, spike, up, rifle, yosef, for\n",
      "and: spike, raw, to, biker, daniel, hammers, the, UNK\n",
      "two: bottom, scenes, during, robbery, area, flashbacks, hour, men\n",
      "way: boys, ground, empire, responsibility, british, templars, pair, writes\n",
      "UNK: in, the, aristocratic, aim, mules, on, centres, biker\n",
      "manages: try, save, causing, gain, attempts, join, doorman, bring\n",
      "from: after, shaft, block, strange, p, letter, strict, successful\n",
      "have: could, hire, become, loved, about, interned, n't, suggesting\n",
      "eva: vel, rrts, vesper, pilate, anthea, stain, interrogations, inclined\n",
      "an: com, american, section, judo, reputation, lookalike, wheelchair, lou\n",
      "former: young, between, businessman, singer, fairy, half, stalked, heart\n",
      "to: ecstasy, and, hammers, spike, up, rifle, yosef, for\n",
      "============================== \n",
      "\n",
      "Loss in epoch 4: 3.224632501602173\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: creatures, interned, could, them, skin, replies, equipment, reanimated\n",
      "apartment: angela, visits, regional, nicky, pants, indifference, house, vicky\n",
      "in: quinn, sequence, performs, dylan, woody, song, bob, guthrie\n",
      "was: ezra, body, had, user, rebecca, contains, also, is\n",
      "UNK: dylan, bob, jones, as, song, roles, played, bruce\n",
      "panics: maneuvers, hash, misleads, wreaks, fievel, immerses, flagged, rental\n",
      "lead: care, village, close, events, lovers, bollywood, drama, low\n",
      "former: named, presents, young, e, massive, neighbor, garden, nurse\n",
      "to: and, roseanna, suribachi, patty, giving, donwynn, johnse, uses\n",
      "and: to, the, glimpses, song, UNK, bruce, dylan, preventing\n",
      "two: efforts, different, style, hull, despondent, arrival, cultural, suits\n",
      "way: end, stage, location, crowd, souls, wanted, remaining, split\n",
      "UNK: dylan, bob, jones, as, song, roles, played, bruce\n",
      "manages: due, try, decides, give, doorman, surrender, attempts, hires\n",
      "from: into, on, in, after, weeping, col, strange, block\n",
      "have: creatures, interned, could, them, skin, replies, equipment, reanimated\n",
      "eva: vel, rrts, vesper, pilate, anthea, stain, interrogations, inclined\n",
      "an: guthrie, performs, dr, glimpses, a, american, thin, as\n",
      "former: named, presents, young, e, massive, neighbor, garden, nurse\n",
      "to: and, roseanna, suribachi, patty, giving, donwynn, johnse, uses\n",
      "============================== \n",
      "\n",
      "Loss in epoch 5: 3.2487387657165527\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: interned, got, could, chloe, replies, asked, says, never\n",
      "apartment: house, home, jersey, returns, replacement, uses, psychotic, room\n",
      "in: mitzvah, harold, from, work, slaps, with, 's, killers\n",
      "was: had, jin, night, has, walked, protests, body, knife\n",
      "UNK: sarkar, slaps, and, min, mitzvah, influenced, is, joy\n",
      "panics: grown, maude, met, assumes, snow, trish, makham, new\n",
      "lead: interviews, british, position, centers, midway, consists, 19th, opposition\n",
      "former: ex, rich, advertising, leon, either, evelyn, huge, daughter\n",
      "to: lassiter, and, killers, beer, nephew, hideous, kuncel, incidents\n",
      "and: chaitali, nephew, encouragement, to, vishnu, killers, harold, butler\n",
      "two: efforts, tanks, different, women, boyfriends, men, tasked, cultural\n",
      "way: question, prop, moon, finish, helped, road, collected, authorities\n",
      "UNK: sarkar, slaps, and, min, mitzvah, influenced, is, joy\n",
      "manages: california, prevent, decides, seduce, encourage, talk, rob, attempts\n",
      "from: sparked, successful, in, taking, knife, currently, after, with\n",
      "have: interned, got, could, chloe, replies, asked, says, never\n",
      "eva: vel, rrts, vesper, pilate, anthea, stain, interrogations, inclined\n",
      "an: preston, a, locates, woody, american, enlisted, ki, '\n",
      "former: ex, rich, advertising, leon, either, evelyn, huge, daughter\n",
      "to: lassiter, and, killers, beer, nephew, hideous, kuncel, incidents\n",
      "============================== \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 6: 3.184863805770874\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: could, maid, were, more, be, skin, may, social\n",
      "apartment: home, indifference, regional, repeated, steve, bedroom, shoes, presence\n",
      "in: 1932, the, of, coup, suppress, a, is, and\n",
      "was: news, reveals, user, drake, knows, body, telescope, accepted\n",
      "UNK: kurt, von, hitler, acquiesces, helene, infatuated, ernst, and\n",
      "panics: divorced, grown, maude, met, snow, trish, makham, cecilia\n",
      "lead: interviews, original, british, centers, midway, 19th, bulldog, bond\n",
      "former: pilot, de, associate, cody, whose, bundle, mr, neighbor\n",
      "to: röhm, putsch, sa, johnson, kahr, hindenburg, resign, replenish\n",
      "and: hitler, von, coup, kurt, hindenburg, 1932, röhm, UNK\n",
      "two: wicker, bounces, chairs, cats, cards, bingo, climbs, autumn\n",
      "way: master, line, moon, woods, race, perform, problem, morning\n",
      "UNK: kurt, von, hitler, acquiesces, helene, infatuated, ernst, and\n",
      "manages: try, stay, decides, wanting, po, manage, california, leave\n",
      "from: che, by, after, outskirts, hating, upon, for, emerges\n",
      "have: could, maid, were, more, be, skin, may, social\n",
      "eva: acquainted, ideology, braun, ambitions, hitler, kahr, favour, abusive\n",
      "an: sherwood, magazine, fashioned, old, delivering, its, 2011, kaye\n",
      "former: pilot, de, associate, cody, whose, bundle, mr, neighbor\n",
      "to: röhm, putsch, sa, johnson, kahr, hindenburg, resign, replenish\n",
      "============================== \n",
      "\n",
      "Loss in epoch 7: 3.225917100906372\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: funds, could, be, interned, exclaims, must, hence, or\n",
      "apartment: jersey, repeated, bedroom, regional, indifference, steve, manipulates, sneaks\n",
      "in: of, beethoven, sergeant, petal, michelangelo, hiking, autopsy, extended\n",
      "was: news, fact, gets, owns, full, user, night, reveals\n",
      "UNK: fluke, wrists, hiking, and, of, locks, the, a\n",
      "panics: divorced, maude, grown, snow, trish, makham, cecilia, activity\n",
      "lead: different, original, centers, british, bulldog, interviews, insurance, bond\n",
      "former: named, neighbor, smith, de, shifty, mixed, michael, bundle\n",
      "to: maurice, apostles, the, towering, and, beast, marco, mirror\n",
      "and: the, with, beast, wrists, UNK, locks, of, to\n",
      "two: vehicles, closets, autumn, lounge, chairs, landscape, women, pace\n",
      "way: ruse, problem, audience, clear, supposed, race, office, entertainment\n",
      "UNK: fluke, wrists, hiking, and, of, locks, the, a\n",
      "manages: prevent, travel, agrees, use, try, plan, issued, stay\n",
      "from: after, alien, autopsy, then, rowing, cookie, carlsen, performs\n",
      "have: funds, could, be, interned, exclaims, must, hence, or\n",
      "eva: acquainted, ideology, ren, braun, ambitions, satyavati, liu, refers\n",
      "an: a, faster, chances, new, little, companion, cite, ambitions\n",
      "former: named, neighbor, smith, de, shifty, mixed, michael, bundle\n",
      "to: maurice, apostles, the, towering, and, beast, marco, mirror\n",
      "============================== \n",
      "\n",
      "Loss in epoch 8: 3.2082395553588867\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: space, thorns, funds, could, impresses, may, interned, aliens\n",
      "apartment: jersey, repeated, bedroom, regional, victoria, cabin, indifference, sneaks\n",
      "in: mose, the, synopsis, a, wide, and, 1930, graveyard\n",
      "was: luka, teja, contains, did, knows, said, user, feels\n",
      "UNK: the, of, mose, and, musicians, a, graveyard, exterminator\n",
      "panics: divorced, snow, trish, makham, cecilia, activity, disliked, harold\n",
      "lead: insurance, interviews, carlino, basic, turk, reddy, centers, british\n",
      "former: mixed, named, de, neighbor, rottweiler, fighter, breed, william\n",
      "to: and, the, sulley, yakuza, arm, hunter, extinct, power\n",
      "and: the, to, acme, of, UNK, extinct, yakuza, sarah\n",
      "two: boys, relationships, difficulties, ambassador, begin, police, hanging, men\n",
      "way: moon, turn, earth, side, present, chase, pretext, head\n",
      "UNK: the, of, mose, and, musicians, a, graveyard, exterminator\n",
      "manages: causing, prevent, travel, plan, escort, issued, give, giving\n",
      "from: popular, maintenance, magnificent, using, by, unleashing, belongs, psychiatrist\n",
      "have: space, thorns, funds, could, impresses, may, interned, aliens\n",
      "eva: ideology, ren, braun, ambitions, satyavati, acquainted, liu, refers\n",
      "an: herself, music, smuggling, telekinetic, works, ridding, agent, california\n",
      "former: mixed, named, de, neighbor, rottweiler, fighter, breed, william\n",
      "to: and, the, sulley, yakuza, arm, hunter, extinct, power\n",
      "============================== \n",
      "\n",
      "Loss in epoch 9: 3.2235395908355713\n",
      "\n",
      " ==========  Similarities summary  ==========\n",
      "have: means, rodent, space, cooperate, interned, balancing, are, inform\n",
      "apartment: indifference, regional, nicky, pants, repeated, due, berta, steve\n",
      "in: of, tv, the, americans, cube, a, after, pair\n",
      "was: leaked, contains, glass, toxic, hart, french, vile, episode\n",
      "UNK: the, marty, mentality, sid, horatio, hacks, kinkaid, of\n",
      "panics: cecilia, trish, divorced, mitzvah, soup, wore, chloe, disability\n",
      "lead: carlino, insurance, basic, turk, bond, reddy, centers, terminal\n",
      "former: rottweiler, smith, mixed, named, breed, foul, de, fighter\n",
      "to: foursome, will, the, that, and, UNK, jason, harmless\n",
      "and: the, worth, crippled, screaming, penrose, hacks, that, to\n",
      "two: exonerated, assassination, ashram, screen, ensuing, awkward, boys, pakistan\n",
      "way: destination, newlyweds, land, wally, truck, locate, battery, corpses\n",
      "UNK: the, marty, mentality, sid, horatio, hacks, kinkaid, of\n",
      "manages: escort, prevent, tries, causing, warden, issued, gain, encourage\n",
      "from: mounted, by, befriend, viciously, hacks, emotional, friction, guard\n",
      "have: means, rodent, space, cooperate, interned, balancing, are, inform\n",
      "eva: ideology, ren, braun, ambitions, satyavati, acquainted, liu, hitler\n",
      "an: crashes, connor, bungalow, mounted, easily, wide, cheetah, ravine\n",
      "former: rottweiler, smith, mixed, named, breed, foul, de, fighter\n",
      "to: foursome, will, the, that, and, UNK, jason, harmless\n",
      "============================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_valid_words(docs, size, tok):\n",
    "    \"\"\" Get a random set of words to check the embeddings \"\"\"\n",
    "    np.random.seed(100)\n",
    "    valid_docs = np.random.choice(docs, size=size//2)\n",
    "    valid_words = []\n",
    "    for doc in valid_docs:\n",
    "        np.random.seed(100)\n",
    "        words = np.random.choice(tok.texts_to_sequences([doc])[0],size=2)\n",
    "        valid_words.extend(words)\n",
    "        \n",
    "    return valid_words\n",
    "\n",
    "\n",
    "def init_tf_variables(sess):\n",
    "    \"\"\" Local and global variable initialization\"\"\"\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    sess.run([init_g,init_l])\n",
    "    \n",
    "    \n",
    "def save_embeddings(sess, filename):\n",
    "    \"\"\" Saving data to disk \"\"\"\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        emb = tf.get_variable(\"embeddings\").eval()\n",
    "        emb_df = pd.DataFrame(emb)\n",
    "        emb_df.insert(0, \"word\", [\"RESERVED\"]+[tok.index_word[w_i] for w_i in range(1,vocab_size)])\n",
    "        emb_df.to_csv(os.path.join('wiki',filename), index=False, header=None)\n",
    "        \n",
    "filename = 'embeddings.csv'    \n",
    "\n",
    "def run_and_eval(\n",
    "    docs, tok, tf_loss, tf_opt, tf_best_inds, tf_in, tf_labels, \n",
    "    vocab_size, batch_size, sampling_table=None, filename='sg_embeddings.csv'\n",
    "):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        init_tf_variables(sess)\n",
    "\n",
    "        valid_words = get_valid_words(docs, 20, tok)\n",
    "\n",
    "        copy_docs = list(docs)\n",
    "\n",
    "        \"\"\" Each epoch \"\"\"\n",
    "        for ep in range(10):\n",
    "            random.shuffle(copy_docs)\n",
    "            losses = []\n",
    "            \"\"\" Each document (i.e. movie plot) \"\"\"\n",
    "            for doc in copy_docs[:50]:            \n",
    "                seq = tok.texts_to_sequences([doc])[0]\n",
    "\n",
    "                \"\"\" Getting skip-gram data \"\"\"\n",
    "                # Negative samples are automatically sampled by tf loss function\n",
    "                \n",
    "                wpairs, labels = skipgrams(\n",
    "                    seq, vocabulary_size=vocab_size,\n",
    "                    negative_samples=0.0\n",
    "                )\n",
    "                \n",
    "                if len(wpairs)==0:\n",
    "                    continue\n",
    "                    \n",
    "                sg_in, sg_out = zip(*wpairs)\n",
    "                assert np.all(np.array(labels)==1)\n",
    "                \n",
    "                    \n",
    "                \"\"\" For each batch in the dataset \"\"\"\n",
    "                for i in range(0, len(sg_in), batch_size):\n",
    "                    \n",
    "                    # TODO: Use sess.run to execute tf_loss and tf_opt operations\n",
    "                    # by feeding in data from i: min(i+batch_size, len(sg_in)) to the graph\n",
    "                    # finally assign the loss to variable l\n",
    "                    loss, _ = sess.run([tf_loss, tf_opt],\n",
    "                            feed_dict={\n",
    "                                tf_in:sg_in[i: min(i+batch_size, len(sg_in))],\n",
    "                                tf_labels:np.reshape(\n",
    "                                    sg_out[i: min(i+batch_size, len(sg_out))],(-1,1))\n",
    "                            })\n",
    "                    losses.append(loss)\n",
    "\n",
    "            \"\"\" Validation phase \"\"\"\n",
    "            \n",
    "            print('Loss in epoch {}: {}'.format(ep, np.mean(losses)))\n",
    "            best_inds = sess.run(tf_best_inds, feed_dict={tf_in:valid_words})\n",
    "\n",
    "            \"\"\" Printing validation data \"\"\"\n",
    "            print('\\n','='*10, ' Similarities summary ', '='*10)\n",
    "            for v_w, ind_list in zip(valid_words, best_inds):\n",
    "                print('{}: {}'.format(tok.index_word[v_w], ', '.join([tok.index_word[b_w] for b_w in ind_list[1:] if b_w!=0])))\n",
    "            print('='*30,'\\n')\n",
    "        \n",
    "        save_embeddings(sess, filename)\n",
    "        \n",
    "run_and_eval(docs, tok, tf_loss, tf_opt, tf_best_inds, tf_in, tf_labels, vocab_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    print('Loading the word embeddings from the disk')\n",
    "    embed_df = pd.read_csv(os.path.join('wiki',filename), index_col=False, header=None)\n",
    "    embed_df = embed_df.set_index(0).iloc[:1000]\n",
    "    print('Embedding shape: {}'.format(embed_df.shape))\n",
    "    embed_mat = embed_df.values\n",
    "    words = embed_df.index.values\n",
    "    return embed_mat, words\n",
    "\n",
    "def get_tsne_embeddings(embed_mat):\n",
    "    print('Running T-SNE')\n",
    "    tsne =  TSNE(n_components=2, perplexity=30, init='pca', n_iter=5000)\n",
    "    tsne_embeds = tsne.fit_transform(embed_mat)\n",
    "    return tsne_embeds\n",
    "\n",
    "def plot_embeddings(tsne_embeds, words):\n",
    "    print('Creating the plot')\n",
    "\n",
    "    fig = plt.figure(figsize=(24,24))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # plot all the embeddings and their corresponding words\n",
    "    for i, (xy, label) in enumerate(zip(tsne_embeds, words)):\n",
    "\n",
    "        ax.scatter(xy[0], xy[1])   \n",
    "        ax.annotate(\n",
    "            label, xy=(xy[0], xy[1]), xytext=(np.random.randint(1), np.random.randint(1)), textcoords='offset points',\n",
    "            ha=np.random.choice(['right','left']), \n",
    "            va=np.random.choice(['bottom','top']),fontsize=10\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "embed_mat, words = load_embeddings('sg_embeddings.csv')\n",
    "tsne_mat = get_tsne_embeddings(embed_mat)\n",
    "plot_embeddings(tsne_mat, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Skip-gram: Improving word vectors\n",
    "\n",
    "* Subsampling\n",
    "* Unigram candidate sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use make_sampling_table function to compute the sampling distribution for each word\n",
    "\n",
    "run_and_eval(docs, tok, tf_loss, tf_opt, tf_best_inds, tf_in, tf_labels, vocab_size, batch_size, sampling_table=sampling_table, filename='sg_sampled_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat, words = load_embeddings('sg_sampled_embeddings.csv')\n",
    "tsne_mat = get_tsne_embeddings(embed_mat)\n",
    "plot_embeddings(tsne_mat, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
