{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "## Introduction\n",
    "In this tutorial you will be learning sentiment analysis. Sentiment analysis is extracting sentiments (i.e. positive, negative, neurtral) from movie reviews, product reviews, social media posts, etc. Often, as these entities are phrases or few sentences we can leverage lstms to learn sentiments from them with the help of a labelled dataset. First we will download and have a look at the data we will be using. We then use a `keras.preprocessing.text.Tokenizer` to preprocess the text. Afterwards we will define a model in Keras that can be used to learn sentiments. Finally we will evaluate the performance of the LSTM model and compared it to the results of a RNN.\n",
    "\n",
    "## Why sentiment analysis\n",
    "Sentiment analysis broadly can be described as follows.\n",
    "\n",
    "> Given a phrase/sentence/short paragraph extracting whether the corpus carry a positive/negative/neutral sentiment\n",
    "\n",
    "From the lense of a model, it might look like below.\n",
    "\n",
    "![Sentiment overview](../images/sentiment_overview.png)\n",
    "\n",
    "### Applications\n",
    "Let's now understand why we need sentiment analysis. Humans are emotional beings. Therefore understanding emotions/sentiments of people is important for delivering good products and services, ensuring their well-being. \n",
    "\n",
    "* Sentiment analysis can be used to extract the negative feedback and group them by the products services. \n",
    "* Sentiment analysis can be used to analyse social media posts and detect if someone is having suicidal thoughts/depression.\n",
    "* Sentiment analysis can be used to compute movie ratings automatically\n",
    "\n",
    "### Challenges\n",
    "* Double negation: `I do not dislike that movie`\n",
    "* Difference in word ordering: `That movie was great, not`\n",
    "* Sarcasm: `I enjoyed the late delivery of it so much!`\n",
    "\n",
    "## Overview of deep sequential models\n",
    "\n",
    "Sentiment analysis can be naturally formulated as a sequential task. Therefore to solve this task sequential models can be used. Sequential models contain a special memory state which allows them to look at sequence of data, while remembering past data. Long Short-Term Memory (LSTM) cell is one of the most-renowned sequental model. Here we will look at LSTM in more detail.\n",
    "\n",
    "### High-level overview of an LSTM cell\n",
    "\n",
    "On high-level an LSTM cell will look like below. First, LSTM cell has following components:\n",
    "* Input: an input coming into the cell (e.g. a word vector)\n",
    "* Cell state: the interenal cell state (i.e. memory)\n",
    "* Output/Hidden state: a hidden state used to compute the final prediction\n",
    "* Input gate: Determines how much of current input read into the cell state\n",
    "* Forget gate: Determines how much of previous cell state (t-1) is read into the current cell state (t)\n",
    "* Output gate: How much of the cell state filters through as the output\n",
    "\n",
    "![LSTM high-level](../images/abstract_lstm.png)\n",
    "\n",
    "### Diving deep into an LSTM cell\n",
    "\n",
    "Below we show the full computations for a LSTM cell.\n",
    "\n",
    "![LSTM complete](../images/lstm_architecture_complete.png)\n",
    "\n",
    "## Implementing an LSTM based sentiment analyser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests \n",
    "import tarfile\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import tarfile\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, SimpleRNN, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 reviews\n",
      "Found 12500 positive reviews and 12500 negative reviews\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "def download_movie_reviews(url):\n",
    "    \"\"\" Downloading and loading the data \"\"\"\n",
    "    \n",
    "    def read_tar(file):\n",
    "        \"\"\" Try loading the data if it is already downloaded \"\"\"\n",
    "        text_data, labels = [],[]\n",
    "        tar = tarfile.open(file, \"r:gz\")\n",
    "        for member in tar.getmembers():\n",
    "                \n",
    "            if re.match(r\"aclImdb\\/train\\/(?:pos|neg)\\/\\w+.txt\",member.name):\n",
    "                text = tar.extractfile(member).read().decode('utf-8')\n",
    "                if member.name.startswith('aclImdb/train/pos'):\n",
    "                    labels.append(1)\n",
    "                    text_data.append(text)\n",
    "                elif member.name.startswith('aclImdb/train/neg'):\n",
    "                    labels.append(0)\n",
    "                    text_data.append(text)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        return text_data, labels\n",
    "    \n",
    "    file = os.path.join('movie-review-data', \"aclImdb_v1.tar.gz\")\n",
    "    \n",
    "    try:\n",
    "        \"\"\" Try loading the data if it is already downloaded \"\"\"\n",
    "        return read_tar(file)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        \"\"\" If the file is not there, download the file \"\"\"\n",
    "        res = requests.get(url, stream=True)\n",
    "        print('Making a directory movie-review-data to store data')\n",
    "        if not os.path.exists('movie-review-data'):\n",
    "            os.mkdir('movie-review-data')\n",
    "        with open(file, 'wb') as f:\n",
    "            print('Downloading data')\n",
    "            f.write(res.content)\n",
    "            print('Download finished.')\n",
    "        \n",
    "        return read_tar(file)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "        \n",
    "orig_text_data, orig_labels = download_movie_reviews(dataset_url)\n",
    "print('Found {} reviews'.format(len(orig_text_data)))\n",
    "print('Found {} positive reviews and {} negative reviews'.format(\n",
    "    sum(orig_labels), len(orig_labels)-sum(orig_labels))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample and shuffle data\n",
    "\n",
    "Here we will shuffle our dataset and extract only `10000` reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 reviews\n",
      "\n",
      "Found 4994 positive reviews and 5006 negative reviews\n",
      "\n",
      "Sample data \n",
      "\t Even for the cocaine laced 1980's this is a pathetic. I don't understand why someone would want to w\n",
      "\t Sometimes it takes a film-making master like Kubrick to bring that extra little something, that uniq\n",
      "\t It's so sad that Romanian audiences are still populated with vulgar and uneducated individuals who r\n",
      "\t Several young Iranian women dress as boys and try to get into a World Cup qualifying match between I\n",
      "\n",
      "Some positive reviews\n",
      "Sometimes it takes a film-making master like Kubrick to bring that extra little something, that uniq\n",
      "A classy film pulled in 2 directions. To its advantage it is directed by Wes Craven. On the downside\n",
      "I find this movie very enjoyable. The plot is simple and easily digestible, the humour is light and \n",
      "\n",
      "Some negative reviews\n",
      "Even for the cocaine laced 1980's this is a pathetic. I don't understand why someone would want to w\n",
      "I watched this hoping to find out something I didn`t know about modern history`s most infamous man a\n",
      "Saw this movie when it first came out in the 1970's and hated, hated, hated it! Easily the most booo\n"
     ]
    }
   ],
   "source": [
    "text_data, labels = list(orig_text_data), list(orig_labels)\n",
    "data_size =10000\n",
    "\n",
    "# Shuffling data\n",
    "random.Random(100).shuffle(text_data)\n",
    "random.Random(100).shuffle(labels)\n",
    "\n",
    "# Removing newline characters with \n",
    "text_data = [t.replace('\\n','') for t in text_data[:data_size]]\n",
    "labels = labels[:data_size]\n",
    "\n",
    "# Printing some stats\n",
    "print('Found {} reviews'.format(len(text_data)))\n",
    "print('\\nFound {} positive reviews and {} negative reviews'.format(\n",
    "    sum(labels), len(labels)-sum(labels))\n",
    "     )\n",
    "\n",
    "print('\\nSample data ')\n",
    "print('\\t', text_data[0][:100])\n",
    "print('\\t', text_data[1][:100])\n",
    "print('\\t', text_data[5000][:100])\n",
    "print('\\t', text_data[5001][:100])\n",
    "\n",
    "# Printing some reviews (positive)\n",
    "print('\\nSome positive reviews')\n",
    "i = 0\n",
    "for rev, lbl in zip(text_data, labels):\n",
    "    if i==3:\n",
    "        break\n",
    "    if lbl==1:\n",
    "        print(rev[:100])\n",
    "        i+=1\n",
    "\n",
    "# Printing some reviews (negative)\n",
    "print('\\nSome negative reviews')\n",
    "i = 0\n",
    "for rev, lbl in zip(text_data, labels):\n",
    "    if i==3:\n",
    "        break\n",
    "    if lbl==0:\n",
    "        print(rev[:100])\n",
    "        i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Tokenizer\n",
    "\n",
    "Here we define several hyperparameters and  we fit a tokenizer with the training data. This tokenizer will be used to tokenize test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2500 # Vocabulary size\n",
    "timesteps = 100 # Number of timesteps\n",
    "train_size = 8000 # Train data size\n",
    "\n",
    "\"\"\" Defining and fitting a tokenizer \"\"\"\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='UNK')\n",
    "\n",
    "tokenizer.fit_on_texts(text_data[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have 10000 reviews.\n",
      "\n",
      "Printing some reviews...\n",
      "Even for the cocaine laced 1980's this is a pathetic. I don't understand why someone would want to w\n",
      "Sometimes it takes a film-making master like Kubrick to bring that extra little something, that uniq\n",
      "\n",
      "Printing some sequences\n",
      "[  1   3   5 259   2   1 131   8   8  73]\n",
      "[  99    1    5 1609   34  857   17   52  112    1]\n",
      "\n",
      "Found 4994 positve and 5006 negative labels\n"
     ]
    }
   ],
   "source": [
    "# Convert all texts to sequences\n",
    "review_sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to a specific length\n",
    "pad_reviews = pad_sequences(\n",
    "    review_sequences, padding='pre', maxlen=timesteps)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Let's print some data\n",
    "print('\\nWe have {} reviews.'.format(len(text_data)))\n",
    "\n",
    "print('\\nPrinting some reviews...')\n",
    "print(text_data[0][:100])\n",
    "print(text_data[1][:100])\n",
    "\n",
    "print('\\nPrinting some sequences')\n",
    "print(pad_reviews[1][:10])\n",
    "print(pad_reviews[5][:10])\n",
    "\n",
    "print(\"\\nFound {} positve and {} negative labels\".format(np.sum(labels==1.0), np.sum(labels==0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w_i (InputLayer)             (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embeddings_i (Embedding)     (None, 100, 128)          320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 451,842\n",
      "Trainable params: 451,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Clearing and resetting the session \"\"\"\n",
    "K.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "hidden_size = 128\n",
    "\n",
    "\"\"\" Defining model layers \"\"\"\n",
    "# Defining input layer\n",
    "inputs = Input(shape=(timesteps,),name='w_i')\n",
    "\n",
    "# Defining the embedding layer\n",
    "embeds = Embedding(\n",
    "    vocab_size, 128, input_length=timesteps, name='embeddings_i'\n",
    ")(inputs)\n",
    "\n",
    "# Defining the LSTM layer\n",
    "lstm_out = LSTM(\n",
    "    hidden_size, return_sequences=False, \n",
    "    return_state=False, dropout=0.5)(embeds)\n",
    "\n",
    "# Defining the dense layer\n",
    "preds = Dense(2, activation='softmax')(lstm_out)\n",
    "\n",
    "\"\"\" Defining and compiling the model \"\"\"\n",
    "model = Model(inputs=inputs, outputs=preds)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tekbac.deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for epoch 1: 77.3\n",
      "Train loss for epoch 1: 0.619250592544675\n",
      "Test accuracy for epoch 2: 81.35\n",
      "Train loss for epoch 2: 0.3642305309781805\n",
      "Test accuracy for epoch 3: 83.05\n",
      "Train loss for epoch 3: 0.2993952558021992\n",
      "Test accuracy for epoch 4: 81.60000000000001\n",
      "Train loss for epoch 4: 0.2552976976558566\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "for ep in range(5):\n",
    "    \n",
    "    accuracy = []\n",
    "    train_losses = []\n",
    "    \"\"\" Training phase \"\"\"\n",
    "    for bi in range(0, train_size, batch_size):\n",
    "        # Getting one batch of inputs and outputs\n",
    "        inp  = pad_reviews[bi:bi+batch_size,:]\n",
    "        out = to_categorical(labels[bi:bi+batch_size], num_classes=2)\n",
    "        # Training the model\n",
    "        model.train_on_batch(inp, out)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, acc= model.evaluate(inp, out, verbose=0)\n",
    "        \n",
    "        train_losses.append(loss)\n",
    "    \n",
    "    \"\"\" Test phase \"\"\"\n",
    "    for bi in range(train_size, data_size, batch_size):\n",
    "        # Getting one batch of inputs and outputs\n",
    "        inp  = pad_reviews[bi:bi+batch_size,:]\n",
    "        out = to_categorical(labels[bi:bi+batch_size], num_classes=2)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        l, acc= model.evaluate(inp, out, verbose=0)\n",
    "        accuracy.append(acc)\n",
    "    \n",
    "    # Printing epoch statistics\n",
    "    print('Test accuracy for epoch {}: {}'.format(ep+1, np.mean(accuracy)*100.0))\n",
    "    print('Train loss for epoch {}: {}'.format(ep+1, np.mean(train_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining RNN-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "hidden_size = 128\n",
    "\n",
    "\"\"\" Defining model layers \"\"\"\n",
    "# Defining input layer\n",
    "inputs = Input(shape=(timesteps,),name='w_i')\n",
    "\n",
    "# Defining the embedding layer\n",
    "embeds = Embedding(vocab_size, 128, input_length=timesteps, name='embeddings_i')(inputs)\n",
    "\n",
    "# Defining the LSTM layer\n",
    "lstm_out = SimpleRNN(hidden_size, return_sequences=False, return_state=False)(embeds)\n",
    "\n",
    "# Defining the dense layer\n",
    "preds = Dense(2, activation='softmax')(lstm_out)\n",
    "\n",
    "\"\"\" Defining and compiling the model \"\"\"\n",
    "model = Model(inputs=inputs, outputs=preds)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN based Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "for ep in range(5):\n",
    "    \n",
    "    rnn_accuracy = []\n",
    "    rnn_train_losses = []\n",
    "    \n",
    "    \"\"\" Training phase \"\"\"\n",
    "    for bi in range(0, train_size, batch_size):\n",
    "        inp, out = pad_reviews[bi:bi+batch_size,:], to_categorical(labels[bi:bi+batch_size], num_classes=2)\n",
    "        model.train_on_batch(inp, out)\n",
    "        l, acc= model.evaluate(inp, out, verbose=0)\n",
    "        rnn_train_losses.append(l)\n",
    "        \n",
    "    \"\"\" Testing phase \"\"\"\n",
    "    for bi in range(train_size, data_size, batch_size):\n",
    "        inp, out = pad_reviews[bi:bi+batch_size,:], to_categorical(labels[bi:bi+batch_size], num_classes=2)\n",
    "        l, acc= model.evaluate(inp, out, verbose=0)\n",
    "        rnn_accuracy.append(acc)\n",
    "    \n",
    "    # Printing epoch statistics\n",
    "    print('Test accuracy for epoch {}: {}'.format(ep+1, np.mean(rnn_accuracy)*100.0))\n",
    "    print('Train loss for epoch {}: {}'.format(ep+1, np.mean(rnn_train_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting LSTM vs RNN accuracies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting LSTM accuracy\n",
    "plt.subplots(2,1, figsize=(24,16))\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(len(accuracy)), accuracy, color='r', label='LSTM')\n",
    "plt.title('Accuracy of LSTM sentiment analyser')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plotting RNN vs LSTM accuracy\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(len(accuracy)), accuracy, color='r', label='LSTM')\n",
    "plt.plot(np.arange(len(rnn_accuracy)), rnn_accuracy , color='b', label='RNN')\n",
    "plt.title('Accuracy of LSTM/RNN sentiment analyser')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
